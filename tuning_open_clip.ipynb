{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azernik/semeval_2025_task1/blob/main/tuning_open_clip.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCwMhir9e3ML"
      },
      "source": [
        "### setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BzSx1mtsu8b"
      },
      "outputs": [],
      "source": [
        "for downloading results from Drive\n",
        "!pip install -q gdown\n",
        "\n",
        "import gdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v_IG3pcVvXr9"
      },
      "outputs": [],
      "source": [
        "!pip install open_clip_torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "udfv44jGJC1x"
      },
      "outputs": [],
      "source": [
        "download taskA file from Adam's Drive (public) and unzip\n",
        "file_id = \"105JdQU_u98w_xSYaNNSj-r4RsyTPXZEF\"\n",
        "url = f\"https://drive.google.com/uc?id={file_id}\"\n",
        "gdown.download(url, \"taskA.zip\", quiet=True)\n",
        "! unzip -q - taskA.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "IEpy5AG8vexN"
      },
      "outputs": [],
      "source": [
        "import open_clip\n",
        "from PIL import Image\n",
        "\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "from ast import literal_eval\n",
        "import requests\n",
        "import json\n",
        "import csv\n",
        "import re\n",
        "from itertools import combinations\n",
        "\n",
        "from scipy.stats import spearmanr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "AxmBGIhbJe1I"
      },
      "outputs": [],
      "source": [
        "# define locations\n",
        "taska_folder = \"train\"\n",
        "taska_tsv_filename = \"subtask_a_train.tsv\"\n",
        "\n",
        "# load data\n",
        "df = pd.read_csv(f\"{taska_folder}/{taska_tsv_filename}\", delimiter=\"\\t\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "DC5Pn-j1v1p2"
      },
      "outputs": [],
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "MQ5BFNR4KYLC"
      },
      "outputs": [],
      "source": [
        "# Preprocess dataframe (image paths, etc.)\n",
        "image_name_cols = ['image1_name', 'image2_name', 'image3_name', 'image4_name', 'image5_name']\n",
        "df['image_paths'] = df.apply(lambda row: [os.path.join(taska_folder, row['compound'].replace(\"'\", \"_\"), row[image_name]) for image_name in image_name_cols], axis=1)\n",
        "df['image_idx_map'] = df.apply(lambda row: {row[name]: i for i, name in enumerate(image_name_cols)}, axis=1)\n",
        "df['expected_order_indices'] = df.apply(lambda row: [row['image_idx_map'][name] for name in literal_eval(row['expected_order'])], axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xC-RWElvv7bL"
      },
      "outputs": [],
      "source": [
        "sentences = df.sentence\n",
        "compounds = df.compound.apply(lambda x: x.replace(\"'\", \"_\"))\n",
        "targets = [literal_eval(t) for t in df.expected_order]\n",
        "s_types = df.sentence_type\n",
        "image_paths = df['image_paths']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGNSR8_0eocY"
      },
      "source": [
        "### evaluation methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IRN5vxPMJtvE"
      },
      "outputs": [],
      "source": [
        "def evaluate_predictions(predictions, df, weights=[0.4, 0.3, 0.2, 0.1, 0.0]):\n",
        "    \"\"\"\n",
        "    Takes predictions, returns three types of evaluation metrics:\n",
        "    - Top-1 Accuracy\n",
        "    - Average Spearman Correlation\n",
        "    - Average Weighted Accuracy\n",
        "    \"\"\"\n",
        "    correct_top1 = 0\n",
        "    spearman_scores, weighted_scores = [], []\n",
        "\n",
        "    for i in range(len(predictions)):\n",
        "        # if len(predictions[i]) == 0:\n",
        "        #     continue\n",
        "\n",
        "        # Ground truth and predictions\n",
        "        # pred_order = [df['image_idx_map'].iloc[i][os.path.basename(df['image_paths'].iloc[i][j])] for j in predictions[i]]\n",
        "        pred_order = predictions[i]\n",
        "        ground_truth_order = df['expected_order_indices'].iloc[i]\n",
        "\n",
        "        # Top-1 accuracy\n",
        "        if pred_order[0] == ground_truth_order[0]:\n",
        "            correct_top1 += 1\n",
        "\n",
        "        # Spearman correlation\n",
        "        score, _ = spearmanr(pred_order, ground_truth_order)\n",
        "        spearman_scores.append(score)\n",
        "\n",
        "        # Weighted accuracy\n",
        "        weighted_score = sum(weights[j] for j, img in enumerate(pred_order) if img == ground_truth_order[j])\n",
        "        weighted_scores.append(weighted_score)\n",
        "\n",
        "    return {\n",
        "        \"top1_accuracy\": correct_top1 / len(predictions),\n",
        "        \"average_spearman\": sum(spearman_scores) / len(spearman_scores),\n",
        "        \"average_weighted_accuracy\": sum(weighted_scores) / len(weighted_scores),\n",
        "        \"spearman_scores\": spearman_scores,\n",
        "        \"weighted_scores\": weighted_scores\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "hzjdIlTfs7r5"
      },
      "outputs": [],
      "source": [
        "def save_results(experiment_name, base_model, model_name, metrics, results_file=\"experiment_results.csv\"):\n",
        "    \"\"\"\n",
        "    Save experiment results to a CSV file.\n",
        "    \"\"\"\n",
        "    # Add experiment name to metrics\n",
        "    results_row = {\n",
        "        \"base_model\": base_model,\n",
        "        \"model\": model_name,\n",
        "        \"experiment\": experiment_name,\n",
        "        \"top1_accuracy\": metrics[\"top1_accuracy\"],\n",
        "        \"average_spearman\": metrics[\"average_spearman\"],\n",
        "        \"average_weighted_accuracy\": metrics[\"average_weighted_accuracy\"],\n",
        "    }\n",
        "\n",
        "    # Write results to CSV\n",
        "    write_header = not os.path.exists(results_file)\n",
        "    with open(results_file, mode=\"a\", newline=\"\") as f:\n",
        "        writer = csv.DictWriter(f, fieldnames=results_row.keys())\n",
        "        if write_header:\n",
        "            writer.writeheader()\n",
        "        writer.writerow(results_row)\n",
        "\n",
        "    print(f\"Results saved to {results_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "cq8YWQHEtIfp"
      },
      "outputs": [],
      "source": [
        "def save_predictions(df, image_paths, predictions, confidence_scores, metrics, prefix, preds_dir='predictions'):\n",
        "    \"\"\"\n",
        "    Save detailed predictions and confidence scores for each example.\n",
        "    \"\"\"\n",
        "    # create 'preds' directory if doesn't exist\n",
        "    if not os.path.exists(preds_dir):\n",
        "        os.makedirs(preds_dir)\n",
        "\n",
        "    # generate output filename\n",
        "    prefix = prefix.strip().replace(\" \", \"_\")\n",
        "    prefix = re.sub(r'[^a-zA-Z0-9_-]', '', prefix)\n",
        "    output_path = f\"{preds_dir}/{prefix}_preds.csv\"\n",
        "\n",
        "    spearman_scores = metrics[\"spearman_scores\"]\n",
        "    weighted_scores = metrics[\"weighted_scores\"]\n",
        "    with open(output_path, mode=\"w\", newline=\"\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        writer.writerow([\"index\", \"compound\", \"ground_truth_order\", \"predicted_order\", \"top1_score\", \"spearman_score\", \"weighted_score\", \"confidence_scores\"])\n",
        "\n",
        "        for i, (pred, conf) in enumerate(zip(predictions, confidence_scores)):\n",
        "            # pred_order = [df['image_idx_map'].iloc[i][os.path.basename(image_paths.iloc[i][j])] for j in pred]\n",
        "            pred_order = pred\n",
        "            ground_truth_order = df[\"expected_order_indices\"].iloc[i]\n",
        "            top1_score = 1 if pred_order[0] == ground_truth_order[0] else 0\n",
        "            spearman_score = round(spearman_scores[i], 3)\n",
        "            weighted_score = round(weighted_scores[i], 3)\n",
        "            formatted_conf_scores = [round(c.item(), 3) for c in conf]\n",
        "            writer.writerow([i, df[\"compound\"].iloc[i], ground_truth_order, pred_order, top1_score, spearman_score, weighted_score, formatted_conf_scores])\n",
        "\n",
        "    print(f\"Predictions saved to {output_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "CBV5Udz_vj2g"
      },
      "outputs": [],
      "source": [
        "def openclip_image_ranking(model, image_processor, tokenizer, image_paths, sentence):\n",
        "    image_inputs = torch.stack([image_processor(Image.open(ipath)) for ipath in image_paths]).to(device)\n",
        "    text_input = tokenizer([sentence]).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image_inputs)\n",
        "        text_features = model.encode_text(text_input)\n",
        "\n",
        "    # normalise features\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # dot product & softmax\n",
        "    similarity = (100.0 * text_features @ image_features.T).softmax(dim=-1)\n",
        "\n",
        "    # order by similarity\n",
        "    probs, indices = similarity[0].topk(5)\n",
        "    return probs, indices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQWrH8QZeuyq"
      },
      "source": [
        "### training and dataset functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "yCNBUqVIYvry"
      },
      "outputs": [],
      "source": [
        "def openclip_train(model, tokenizer, image_preprocess, dataloader, optimizer):\n",
        "    # one epoch only\n",
        "    # image paths are ordered by how similar they should be to the sentence\n",
        "    model.train()\n",
        "    margin = 0.1\n",
        "    for batch in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        texts, imgs1, imgs2 = batch\n",
        "\n",
        "        # encode text and images - always the preferred image is img1\n",
        "        text_input = tokenizer(texts).to(device)\n",
        "\n",
        "        text_features = model.encode_text(text_input, normalize=True)\n",
        "\n",
        "        image_inputs1 = torch.stack([image_preprocess(Image.open(ipath)) for ipath in imgs1]).to(device)\n",
        "        image_features1 = model.encode_image(image_inputs1, normalize=True)\n",
        "\n",
        "        image_inputs2 = torch.stack([image_preprocess(Image.open(ipath)) for ipath in imgs2]).to(device)\n",
        "        image_features2 = model.encode_image(image_inputs2, normalize=True)\n",
        "\n",
        "        # dot product\n",
        "        B, D = text_features.shape\n",
        "        similarities1 = torch.bmm(text_features.view(B, 1, D), image_features1.view(B, D, 1)) # expected to be more similar\n",
        "        similarities1 = similarities1.squeeze(-1)\n",
        "        similarities2 = torch.bmm(text_features.view(B, 1, D), image_features2.view(B, D, 1)) # expected to be less similar\n",
        "        similarities2 = similarities2.squeeze(-1)\n",
        "\n",
        "        # compare logits\n",
        "        contrastive_loss = torch.nn.functional.relu(margin + similarities2 - similarities1).sum() # less - more to give -ve diff and 0 loss if correct\n",
        "\n",
        "        # update params\n",
        "        contrastive_loss.backward()\n",
        "        # print(contrastive_loss)\n",
        "        optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "def openclip_evaluate(model, tokenizer, image_preprocess, test_sentences, test_image_paths, test_targets, verbose=True):\n",
        "    model.eval()\n",
        "    predictions, confidence = [], []\n",
        "    for s, ipaths, tgt in zip(test_sentences, test_image_paths, test_targets):\n",
        "        sorted_probs, ids_sorted = openclip_image_ranking(model, image_preprocess, tokenizer, ipaths, s)\n",
        "        predictions.append(ids_sorted.tolist())\n",
        "        confidence.append(100 * sorted_probs)\n",
        "    return predictions, confidence\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "RypeYIaCprCj"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Custom Dataset\n",
        "class PairwiseDataset(Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text, img1, img2 = self.data[idx]\n",
        "        # Add logic to load image tensors if needed\n",
        "        return text, img1, img2\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "uIYUiA-hpHp1"
      },
      "outputs": [],
      "source": [
        "def reorder_image_paths(ipaths, tgt):\n",
        "    tgt_order = {t:j for j, t in enumerate(tgt)}\n",
        "    ordered_ipaths = sorted(ipaths, key=lambda x: tgt_order[x.split('/')[-1]])\n",
        "    return ordered_ipaths\n",
        "\n",
        "# image_paths are the original order\n",
        "# ordered_image_paths are in the target order, which is useful for training\n",
        "# (but also means if you evaluate performance on the training set using the standard function it looks terrible)\n",
        "\n",
        "image_paths_copy = image_paths.copy()\n",
        "ordered_image_paths = [reorder_image_paths(ipaths, targets[i]) for i, ipaths in enumerate(image_paths_copy)]\n",
        "\n",
        "def split_train_and_test_data(test_indices, train_indices, input_text):\n",
        "    testing_sentences = [input_text[idx] for idx in test_indices]\n",
        "    training_sentences = [input_text[idx] for idx in train_indices]\n",
        "\n",
        "    testing_targets = [targets[idx] for idx in test_indices]\n",
        "    training_targets = [targets[idx] for idx in train_indices]\n",
        "\n",
        "    testing_image_paths = [image_paths[idx] for idx in test_indices]\n",
        "    training_image_paths = [ordered_image_paths[idx] for idx in train_indices]\n",
        "\n",
        "    return {'train': (training_sentences, training_image_paths, training_targets),\n",
        "            'test': (testing_sentences, testing_image_paths, testing_targets)}\n",
        "\n",
        "def make_pairwise(sentences, image_paths):\n",
        "    pairwise_dataset = []\n",
        "    for text, images in zip(sentences, image_paths):\n",
        "        pairs = list(combinations(range(len(images)), 2))  # All 10 pairs\n",
        "        for i, j in pairs:\n",
        "            pairwise_dataset.append((text, images[i], images[j]))\n",
        "    return pairwise_dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5ibqJl9ezzt"
      },
      "source": [
        "### get data and run training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "UT2xvxl2sFZv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1T9pMSMj6JQP0DCLy-6H7dfUtVRWy39uq\n",
            "To: /home/shurui/projects/semeval_2025_task1/gpt_prompt_responses.csv\n",
            "100%|██████████| 30.7k/30.7k [00:00<00:00, 3.98MB/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "'gpt_prompt_responses.csv'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gdown\n",
        "# Download the file of prompt responses from Google Drive\n",
        "gdown.download(\"https://drive.google.com/uc?id=1T9pMSMj6JQP0DCLy-6H7dfUtVRWy39uq\", 'gpt_prompt_responses.csv', quiet=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "5-Ybw-57yugQ"
      },
      "outputs": [],
      "source": [
        "df_text_inputs = pd.read_csv('gpt_prompt_responses.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A9xAzSoOtR0U"
      },
      "source": [
        "Trains two epochs across 10 splits of the data to get results for each sample.\n",
        "The results are collected together then evaluated and saved."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ffTVFDBGsSIG"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0111e8a087524328ab1191cea6cade1a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "open_clip_model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training split 0, epoch 0\n",
            "training split 0, epoch 1\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtraining split \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m     model \u001b[38;5;241m=\u001b[39m openclip_train(model, tokenizer, preprocess_openclip, pairwise_dataloader, optimizer)\n\u001b[1;32m     49\u001b[0m     predictions, confidence \u001b[38;5;241m=\u001b[39m openclip_evaluate(model, tokenizer, preprocess_openclip, \u001b[38;5;241m*\u001b[39msplit_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m], verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j, orig_id \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(test_indices):\n",
            "Cell \u001b[0;32mIn[13], line 16\u001b[0m, in \u001b[0;36mopenclip_train\u001b[0;34m(model, tokenizer, image_preprocess, dataloader, optimizer)\u001b[0m\n\u001b[1;32m     12\u001b[0m text_input \u001b[38;5;241m=\u001b[39m tokenizer(texts)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     14\u001b[0m text_features \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_text(text_input, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 16\u001b[0m image_inputs1 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([image_preprocess(Image\u001b[38;5;241m.\u001b[39mopen(ipath)) \u001b[38;5;28;01mfor\u001b[39;00m ipath \u001b[38;5;129;01min\u001b[39;00m imgs1])\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     17\u001b[0m image_features1 \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mencode_image(image_inputs1, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     19\u001b[0m image_inputs2 \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([image_preprocess(Image\u001b[38;5;241m.\u001b[39mopen(ipath)) \u001b[38;5;28;01mfor\u001b[39;00m ipath \u001b[38;5;129;01min\u001b[39;00m imgs2])\u001b[38;5;241m.\u001b[39mto(device)\n",
            "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
            "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
            "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torchvision/transforms/transforms.py:354\u001b[0m, in \u001b[0;36mResize.forward\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[1;32m    347\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[1;32m    353\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 354\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mresize(img, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msize, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterpolation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mantialias)\n",
            "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torchvision/transforms/functional.py:477\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[1;32m    475\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    476\u001b[0m     pil_interpolation \u001b[38;5;241m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[0;32m--> 477\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F_pil\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39mpil_interpolation)\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F_t\u001b[38;5;241m.\u001b[39mresize(img, size\u001b[38;5;241m=\u001b[39moutput_size, interpolation\u001b[38;5;241m=\u001b[39minterpolation\u001b[38;5;241m.\u001b[39mvalue, antialias\u001b[38;5;241m=\u001b[39mantialias)\n",
            "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/torchvision/transforms/_functional_pil.py:250\u001b[0m, in \u001b[0;36mresize\u001b[0;34m(img, size, interpolation)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(size, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(size) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot inappropriate size arg: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mresize(\u001b[38;5;28mtuple\u001b[39m(size[::\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), interpolation)\n",
            "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/PIL/Image.py:2336\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2333\u001b[0m     im \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mresize(size, resample, box)\n\u001b[1;32m   2334\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m im\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode)\n\u001b[0;32m-> 2336\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m   2338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reducing_gap \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m resample \u001b[38;5;241m!=\u001b[39m Resampling\u001b[38;5;241m.\u001b[39mNEAREST:\n\u001b[1;32m   2339\u001b[0m     factor_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m((box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m size[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m/\u001b[39m reducing_gap) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m1\u001b[39m\n",
            "File \u001b[0;32m~/anaconda3/envs/torch/lib/python3.12/site-packages/PIL/ImageFile.py:300\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    299\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 300\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m decoder\u001b[38;5;241m.\u001b[39mdecode(b)\n\u001b[1;32m    301\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "num_groups = 10\n",
        "order_for_testing = torch.randperm(len(sentences))\n",
        "testing_groups = torch.chunk(order_for_testing, num_groups)\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "all_predictions = {i:\n",
        "    {'preds': [[0,1,2,3,4]]*len(sentences), 'conf': [[0.2,0.2,0.2,0.2,0.2]]*len(sentences)}\n",
        "    for i in range(num_epochs)}\n",
        "\n",
        "experiment_name = 'baseline_sentences'\n",
        "base_model = 'openclip'\n",
        "model_name = 'ViT-B-32_finetune'\n",
        "\n",
        "for i in range(len(testing_groups)):\n",
        "    test_indices = testing_groups[i].tolist()\n",
        "    train_indices = torch.concat(testing_groups[:i] + testing_groups[i+1:]).tolist()\n",
        "    split_data = split_train_and_test_data(test_indices, train_indices, sentences)\n",
        "\n",
        "    pairwise_train_data = make_pairwise(*split_data['train'][:2])\n",
        "\n",
        "    # Initialize Dataset and DataLoader for training\n",
        "    pairwise_Dataset = PairwiseDataset(pairwise_train_data)\n",
        "    pairwise_dataloader = DataLoader(pairwise_Dataset, batch_size=8, shuffle=True)\n",
        "\n",
        "    # prep model\n",
        "    model_openclip, _, preprocess_openclip = open_clip.create_model_and_transforms('ViT-B-32', pretrained='laion2b_s34b_b79k')\n",
        "    model_openclip.to(device)\n",
        "    # model_openclip.eval()  # model in train mode by default, impacts some models with BatchNorm or stochastic depth active\n",
        "    tokenizer = open_clip.get_tokenizer('ViT-B-32')\n",
        "    model_openclip.train()\n",
        "\n",
        "    # don't train the image part of the model\n",
        "    for param in model_openclip.visual.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # # only train the image part of the model\n",
        "    # for name, param in model_openclip.named_parameters():\n",
        "    #     if not name.startswith('visual'):\n",
        "    #         param.requires_grad = False\n",
        "\n",
        "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model_openclip.parameters()), lr=1e-6)\n",
        "\n",
        "    # pre training eval\n",
        "    model = model_openclip\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f'training split {i}, epoch {epoch}')\n",
        "        model = openclip_train(model, tokenizer, preprocess_openclip, pairwise_dataloader, optimizer)\n",
        "        predictions, confidence = openclip_evaluate(model, tokenizer, preprocess_openclip, *split_data['test'], verbose=False)\n",
        "\n",
        "        for j, orig_id in enumerate(test_indices):\n",
        "            all_predictions[epoch]['preds'][orig_id] = predictions[j]\n",
        "            all_predictions[epoch]['conf'][orig_id] = confidence[j]\n",
        "\n",
        "# print(all_predictions)\n",
        "for epoch in range(num_epochs):\n",
        "    preds = all_predictions[epoch]['preds']\n",
        "    conf = all_predictions[epoch]['conf']\n",
        "    results = evaluate_predictions(preds, df)\n",
        "\n",
        "    save_results(experiment_name, base_model, model_name+f'_e{epoch}', results, results_file=\"experiment_results.csv\")\n",
        "\n",
        "    prefix = experiment_name+'_'+base_model+'_'+model_name+f'_e{epoch}'\n",
        "    save_predictions(df, None, preds, conf, results, prefix, preds_dir='predictions')\n",
        "    \n",
        "# save the model\n",
        "torch.save(model.state_dict(), f\"openclip_{model_name}_epoch{num_epochs}.pth\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP-elPrvi4ZZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ykx-qiAI3b4y"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "dCwMhir9e3ML",
        "CGNSR8_0eocY",
        "yQWrH8QZeuyq"
      ],
      "gpuType": "A100",
      "include_colab_link": true,
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
